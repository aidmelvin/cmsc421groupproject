{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6052ca-b958-44ab-b002-d9c4fbb539ae",
   "metadata": {},
   "source": [
    "# Image Generation\n",
    "\n",
    "This notebook includes several code snippets used to generate and display images using Stable Diffusion v1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b128f1cc-c35c-435e-a2f3-e438abbaa6e4",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12543f-0a2c-44a3-98e2-fb250c5192c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from diffusers import DiffusionPipeline\n",
    "import transformers\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64386ffa-0ff4-4cd3-ba3a-af96d0581340",
   "metadata": {},
   "source": [
    "## Load Diffusion Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd90837-cbed-435e-9628-8a06a406fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_captions = load_dataset('lambdalabs/pokemon-blip-captions')\n",
    "dataset = pokemon_captions['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f2f5bc-0586-4960-9f75-4d32a5461af6",
   "metadata": {},
   "source": [
    "Throughout this notebook, we follow the guidance found in the [tutorial](https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation) on HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9c258-d692-44be-afad-f4b78f1f6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = 'REDACTED'\n",
    "generator = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_auth_token='HF_TOKEN')\n",
    "generator.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95bde2-c88d-4883-aa4d-f0d49a06d44c",
   "metadata": {},
   "source": [
    "## Creating Training Dataset\n",
    "\n",
    "We randomly select 80 images from the Pokemon captions dataset and run the captions through the diffusion model, saving them to a folder.\n",
    "\n",
    "We perform this generation in groups of 20 (dividing it up amongst our 4 group members).\n",
    "\n",
    "We set the random seed each time to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9570374-ef87-4d5d-85de-30b364701d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = 'Generated Pokemon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b8280-73dc-4a3e-af4b-4d5cbd7d64b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_if_not_present(i: int):\n",
    "    if os.path.isfile(FOLDER + f\"GenPokemon{i}.png\"):\n",
    "        return\n",
    "    text = dataset[i]['text']\n",
    "    transformers.set_seed(27)\n",
    "    image = generator(text).images[0]\n",
    "    image.save(FOLDER + f\"GenPokemon{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cea760-412a-43bf-ae7d-79220de5e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_if_not_present(i: int):\n",
    "    if os.path.isfile(FOLDER + f\"GenPokemon{i}.png\"):\n",
    "        return\n",
    "    text = dataset[i]['text']\n",
    "    transformers.set_seed(27)\n",
    "    image = generator(text).images[0]\n",
    "    image.save(FOLDER + f\"GenPokemon{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01a7bf-978f-4385-8f39-1b536fc514b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate first group IDs\n",
    "group1 = []\n",
    "while len(group1) < 20:\n",
    "    i = random.randrange(len(dataset))\n",
    "    if i not in group1:\n",
    "        gen_if_not_present(i)\n",
    "        group1.append(i)\n",
    "print(group1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed4efe-b67d-428e-a3a0-f7261719142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate second group IDs\n",
    "group2 = []\n",
    "while len(group2) < 20:\n",
    "    i = random.randrange(len(dataset))\n",
    "    if i not in group1 and i not in group2:\n",
    "        gen_if_not_present(i)\n",
    "        group2.append(i)\n",
    "print(group2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c14d428-a648-4aaf-aed8-1e698634cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate third group IDs\n",
    "group3 = []\n",
    "while len(group3) < 20:\n",
    "    i = random.randrange(len(dataset))\n",
    "    if i not in group1 and i not in group2 and i not in group3:\n",
    "        gen_if_not_present(i)\n",
    "        group3.append(i)\n",
    "print(group3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f4e2c-3bc1-4b88-84b1-4d50fefd8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fourth group IDs\n",
    "group4 = []\n",
    "while len(group4) < 20:\n",
    "    i = random.randrange(len(dataset))\n",
    "    if i not in group1 and i not in group2 and i not in group3 and i not in group4:\n",
    "        gen_if_not_present(i)\n",
    "        group4.append(i)\n",
    "print(group4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ddfaf5-4625-44d3-b189-f1b8620ad110",
   "metadata": {},
   "source": [
    "## Fetch and Display Image\n",
    "\n",
    "The following cell is self-contained so that it can be run by other group members without running the full notebook (as long as they have the images folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075cebce-11e0-4ce7-aab0-b8309e28f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install HuggingFace datasets library\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "pokemon_captions_dataset = load_dataset('lambdalabs/pokemon-blip-captions')\n",
    "dataset = pokemon_captions_dataset['train']\n",
    "INDEX = 105\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "transformers.set_seed(27)\n",
    "gen_image = generator(dataset[INDEX]['text']).images[0]\n",
    "\n",
    "print(dataset[INDEX]['text'])\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[1].imshow(Image.open(f\"Generated Pokemon/GenPokemon{INDEX}.png\"))\n",
    "axarr[1].imshow(gen_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491a940-0ffe-4cc8-acc0-3ca8d86830eb",
   "metadata": {},
   "source": [
    "## Generate with Feedback\n",
    "\n",
    "The following set of cells are also self-contained.\n",
    "\n",
    "The feedback should be subsituted with the desired LLM/VLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002671aa-46a0-4a35-919b-08be3a436e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install HuggingFace datasets library\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import transformers\n",
    "\n",
    "pokemon_captions_dataset = load_dataset('lambdalabs/pokemon-blip-captions')\n",
    "dataset = pokemon_captions_dataset['train']\n",
    "\n",
    "# install PyTorch and HuggingFace diffusers library\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "HF_TOKEN = 'REDACTED' # make a token on Huggingface\n",
    "generator = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_auth_token=HF_TOKEN)\n",
    "generator.to(\"cuda\") # if you can use your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84765dde-499b-4f3f-b9e6-aa71393ebcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# change this to the desired number\n",
    "INDEX = 27\n",
    "\n",
    "# create image caption\n",
    "orig_caption = dataset[INDEX]['text']\n",
    "feedback = 'The Pokemon is a cartoon lady with pink baggy pants and a big hat with a white background.  It has a white background and is smiling against a white background.'\n",
    "# new_caption = orig_caption + '. ' + feedback\n",
    "print(f\"Feedback: {feedback}\")\n",
    "\n",
    "# generate image for caption\n",
    "transformers.set_seed(27)\n",
    "image = generator(feedback).images[0]\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
